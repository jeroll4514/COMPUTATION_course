{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fa647ea",
   "metadata": {},
   "source": [
    "For this notebook you will need\n",
    "\n",
    "+ numpy\n",
    "+ matplotlib\n",
    "+ scipy\n",
    "+ scikit-learn\n",
    "\n",
    "$$\n",
    "\\let\\vaccent=\\v % rename builtin command \\v{} to \\vaccent{}\n",
    "\\renewcommand{\\v}[1]{{\\mathbf{#1}}} % for vectors\n",
    "\\renewcommand{\\gv}[1]{{\\mbox{\\boldmath$ #1 $}}} \n",
    "\\renewcommand{\\uv}[1]{{\\mathbf{\\hat{#1}}}} % for unit vector\n",
    "\\renewcommand{\\abs}[1]{\\left| #1 \\right|} % for absolute value\n",
    "\\renewcommand{\\avg}[1]{\\left< #1 \\right>} % for average\n",
    "% \\let\\underdot=\\d % rename builtin command \\d{} to \\underdot{}\n",
    "\\newcommand{\\d}[2]{\\frac{d #1}{d #2}} % for derivatives\n",
    "\\newcommand{\\dd}[2]{\\frac{d^2 #1}{d #2^2}} % for double derivatives\n",
    "% \\newcommand{\\pd}[2]{\\frac{\\partial #1}{\\partial #2}} \n",
    "% \\renewcommand\\eqref[1]{Eq.\\;\\ref{#1}} % new version of eqref\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ee03d2d-906b-4dee-aee1-b48f2a17a887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929aa97a-fa52-4629-b050-ab5479da6701",
   "metadata": {},
   "source": [
    "# Eigensystem calculations, optimization, and linear dynamical systems\n",
    "\n",
    "### Finding eigenspectra is difficult for large matrices\n",
    "\n",
    "Suppose we are given a square matrix $A \\in \\mathbb{R}^{N \\times N}$. How can we find the eigenvalues and eigenvectors of this matrix numerically? The schoolyard method for performing this calculation consists of first solving the characteristic equation for the eigenvalues $\\lambda$ such that\n",
    "$$\n",
    "\\det(A - \\lambda \\mathbb I) = 0.\n",
    "$$\n",
    "In principle, this equation always factors into a polynomial with solutions corresponding to the eigenvalues $(\\lambda - \\lambda_1)(\\lambda - \\lambda_2)...(\\lambda - \\lambda_N) = 0$. However, as $N$ grows larger, it becomes progressively more difficult to factor the equation and find the roots $\\lambda_i$, since the polynomial has order $\\mathcal{O}(\\lambda^N)$. At large $N$, it often becomes impractical to use numerical root-finding to solve the eigenvalue problem using schoolyard methods.\n",
    "\n",
    "### The power method\n",
    "\n",
    "The [power method](https://www.cs.huji.ac.il/w~csip/tirgul2.pdf) is a purely iterative method for finding the leading eigenvector of large matrices. The basic algorithm is as follows:\n",
    "\n",
    "0. We start with a matrix $A \\in \\mathbb{R}^{N \\times N}$.\n",
    "1. Pick a random vector $\\v{v} \\in \\mathbb{R}^{N}$, and convert it to a unit vector by scaling it with its own norm $\\v{v} \\leftarrow \\v{v}/{||\\v{v}||}$.\n",
    "2. Compute the matrix product of our matrix $A$ with the random unit vector, and then update the vector $\\v{v} \\leftarrow A \\v{v}$.\n",
    "3. Re-normalize the resulting vector, producing a new unit vector, $\\v{v} \\leftarrow \\v{v}/{||\\v{v}||}$\n",
    "4. Repeat steps 2 and 3 until the elements of the output unit vector fluctuate less than a pre-specified tolerance\n",
    "5. Multiply the resulting vector by the original matrix $A$. The length of the resulting vector gives the magnitude of the leading eigenvalue\n",
    "\n",
    "### The power method derivation\n",
    "\n",
    "Suppose we seek to find the leading eigenvector of a matrix $A$. If the matrix $A$ is non-singular, then it has a full-rank eigenbasis $V \\in \\mathbb{R}^{n \\times n}$, spanned by the $N$ independent, orthonormal eigenvectors $\\v{v}_i$ such that $\\v{v}_i \\cdot \\v{v}_j = \\delta_{ij}$. We start with a our random vector $\\v{w}$, and write it in terms of the basis $V$,\n",
    "$$\n",
    "\\v{w} = w_1 \\v{v}_1 + w_2 \\v{v}_2 + ... + w_N \\v{v}_N\n",
    "$$\n",
    "$$\n",
    "A \\v{w} = \\lambda_1 w_1 \\v{v}_i + \\lambda_2 w_2 \\v{v}_2 + ... + \\lambda_N w_N \\v{v}_N\n",
    "$$\n",
    "\n",
    "We next compute the norm of the output vector\n",
    "\n",
    "$$\n",
    "A \\v{w} \\cdot  A \\v{w} = \\lambda_1^2 w_1^2 + \\lambda_1^2 w_1^2 + ... + \\lambda_1^N w_1^N\n",
    "$$\n",
    "for simplicity, we define $C \\equiv \\sqrt{\\lambda_1^2 w_1^2 + \\lambda_1^2 w_1^2 + ... + \\lambda_1^N w_1^N}$. Rescaling our transformed vector by the norm, we apply the matrix $A$ again,\n",
    "$$\n",
    "A \\cdot (A \\cdot \\v{w})/C = (1/C) (\\lambda_1^2 w_1 \\v{v}_i + \\lambda_2^2 w_2 \\v{v}_2 + ... + \\lambda_N^2 w_N \\v{v}_N)\n",
    "$$\n",
    "\n",
    "This quantity has the norm,\n",
    "$$\n",
    "||A \\cdot (A \\cdot \\v{w})/C||^2 = (1/C^2) (\\lambda_1^4 w_1^2 \\v{v}_i + \\lambda_2^4 w_2 \\v{v}_2 + ... + \\lambda_N^4 w_N \\v{v}_N)\n",
    "$$\n",
    "\n",
    "Now we consider the limit as as $M \\rightarrow \\infty$. Without loss of generality, we assume that the $N$ eigenvalues of $A$ are ordered by their magnitude, $|\\lambda_1| > |\\lambda_2| > ... > |\\lambda_N|$. The series above diverges geometrically as we iterate repeatedly, such that\n",
    "\n",
    "$$\n",
    "A^M \\v{w} \\approx \\dfrac{\\lambda_1^M w_1 + ...}{\\sqrt{\\lambda_1^{2M} w_1^2 + ...}} \\v{v}_1 = \\v{v_1}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## To Do\n",
    "\n",
    "1. Implement the power method in Python. I've included my starter code below.\n",
    "2. Sometimes you'll notice that the power method fails to converge to the correct solution. What is special about randomly-sampled matrices where this occurs? How does the direction of the starting vector affect the time it takes to reach a solution? \n",
    "\n",
    "Answer: The power method will fail to converge when the leading two eigenvalues are close to the same value.  This is because when we apply our matrix, all eigenvalues are affecting where our vector is being mapped to.  However, if we repeatedly apply our matrix, only the largest one will be noticable after a large number of iterations (so we can neglect the affects of all other eigenvalues).  But if we have the largest two eigenvalues very close to each other, it takes $\\textit{much}$ longer to only see our largest eigenvalue.\n",
    "\n",
    "The direction of our starting vector will slightly affect the time it takes to reach a solution.  It is converging to the eigenvector, after all, so if we happen to have a great (random) guess then it will be quick.\n",
    "\n",
    "3. Suppose that we interpret a given linear matrix $X$ as describing a discrete-time linear dynamical system, $\\v{y}_{t+1} = X \\v{y}_t$. What kind of dynamics does the power method exhibit? What about the pathological cases you discussed in the previous solution?\n",
    "\n",
    "Answer: Looking at our matrix in this framework, we can think of the power method exhibiting the asymptotic dynamics.  Repeatedly applying our matrix looks further in time, where we see that the largest eigenvalue is the largest contributor to the dynamics.\n",
    "\n",
    "4. The power method represents a basic optimization problem, where we are searching for a convergent solution. We saw that our method occasionally fails to find the correct solution. One way to improve our optimization would be to add a momentum term of the form $$\\v{y}_t \\leftarrow \\gamma \\v{y}_{t - 1} + (1 - \\gamma) \\dfrac{X \\v{y}_{t - 1}} {|X \\v{y}_{t - 1}|}. $$ Where $\\gamma \\in (0, 1]$. How would you modify your implementation of the power method, in order to allow momentum? What kinds of pathological dynamics would the momentum term help us avoid?\n",
    "\n",
    "Answer: We could implement this momentum term by storing both the current vector and the prior vector (currently, I just store the two eigenvalues, not the whole vectors).  But this $\\gamma$ term essentially shifts our eigenvector less for each step.  This can help avoid oscillatory results, where we are constantly overshooting our final destination.\n",
    "\n",
    "5. Similar to the momentum term, there is also a way to add additional damping to the update rule. What kinds of dynamics would that help us avoid?\n",
    "\n",
    "Answer: Additional damping would also help avoid oscillating behavior.  This is evident when looking at the extreme case of overdamping, there is no oscillation then.\n",
    "<!-- 7. *Optional:* Perform a computational experiment in which you compare the schoolyard and power method, to see which $N$ they crossover at. -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d555fdbc-413a-45d1-af62-a66499c0a952",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "class SpectralDecompositionPowerMethod:\n",
    "    \"\"\"\n",
    "    Store the output vector in the object attribute self.components_ and the \n",
    "    associated eigenvalue in the object attribute self.singular_values_ \n",
    "    \n",
    "    Why this code structure and attribute names? We are using the convention used by \n",
    "    the popular scikit-learn machine learning library:\n",
    "    https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "\n",
    "    Parameters\n",
    "        max_iter (int): maximum number of iterations to for the calculation\n",
    "        tolerance (float): fractional change in solution to stop iteration early\n",
    "        gamma (float): momentum parameter for the power method\n",
    "        random_state (int): random seed for reproducibility\n",
    "        store_intermediate_results (bool): whether to store the intermediate results as\n",
    "            the power method iterates\n",
    "        stored_eigenvalues (list): If store_intermediate_results is active, a list of \n",
    "            eigenvalues at each iteration\n",
    "        stored_eigenvectors (list): If store_intermediate_results is active, a list of\n",
    "            eigenvectors at each iteration\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "        max_iter=10000, \n",
    "        tolerance=1e-5, \n",
    "        gamma=0.0,\n",
    "        random_state=None, \n",
    "        store_intermediate_results=False\n",
    "    ):\n",
    "        self.max_iter = max_iter\n",
    "        self.tolerance = tolerance\n",
    "        self.gamma = gamma\n",
    "        self.random_state = random_state\n",
    "        self.store_bool = store_intermediate_results\n",
    "        self.conv = False #will store if result converges or not\n",
    "\n",
    "        if self.store_bool is True:\n",
    "            self.stored_eigenvalues  = [] #this will store intermediate eigenvalues\n",
    "            self.stored_eigenvectors = [] #this will store intermediate eigenvectors\n",
    "\n",
    "\n",
    "    def fit(self, A):\n",
    "        \"\"\"\n",
    "        Perform the power method with random initialization, and optionally store\n",
    "        intermediate estimates of the eigenvalue and eigenvectors at each iteration.\n",
    "        You can add an early stopping criterion based on the tolerance parameter.\n",
    "        \"\"\"\n",
    "        np.random.seed(self.random_state)\n",
    "\n",
    "        v = np.random.rand(A.shape[0])\n",
    "        v = v/np.linalg.norm(v)\n",
    "        len = [0,0] #will store vector magnitudes (compared for convergence)\n",
    "        for ii in range(self.max_iter):\n",
    "\n",
    "            v = A.dot(v)\n",
    "            len[ii%2] = np.linalg.norm(v) #finds magnitude\n",
    "\n",
    "            if self.store_bool is True:\n",
    "                self.stored_eigenvalues.append(len[ii%2])\n",
    "                self.stored_eigenvectors.append(v)\n",
    "\n",
    "            v = v/len[ii%2] #Normalizes vector for next step\n",
    "\n",
    "            diff = np.abs(len[1] - len[0]) #finds difference in prior and new eigenvalue\n",
    "            if diff < self.tolerance: #exits loop if eigenvalue converges\n",
    "                self.conv = True\n",
    "                break\n",
    "\n",
    "        if self.conv is False:\n",
    "            print(f'Warning: did not converge within {self.max_iter} steps')\n",
    "\n",
    "        v = A.dot(v)\n",
    "        self.components_ = v/np.linalg.norm(v) #will store output vector\n",
    "        self.singular_values_ = np.linalg.norm(v) #will store largest eigenvector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb5f409",
   "metadata": {},
   "source": [
    "### Test and use your code\n",
    "\n",
    "+ If you are working from a local fork of the entire course, then you already have access to the solutions. In this case, make sure to `git pull` to make sure that you are up-to-date (save your work first).\n",
    "+ If you are working from a single downloaded notebook, or are working in Google Colab, then you will need to manually download the solutions file from the course repository. The lines below will do this for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4ea13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "# Check if the \"solutions\" directory exists. If not, create it and download the solution file\n",
    "if not os.path.exists('solutions'):\n",
    "    os.makedirs('solutions')\n",
    "else:\n",
    "    print('Directory \"solutions\" already exists. Skipping creation.')\n",
    "\n",
    "# Now download the solution file into the directory we just created\n",
    "url = 'https://raw.githubusercontent.com/williamgilpin/cphy/main/hw/solutions/eigen.py'\n",
    "response = requests.get(url)\n",
    "file_path = os.path.join('solutions', 'sandpile.py')\n",
    "with open(file_path, 'wb') as file:\n",
    "    file.write(response.content)\n",
    "print(f'File saved to {file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4801d3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import William's solutions\n",
    "# from solutions.eigen import SpectralDecompositionPowerMethod\n",
    "\n",
    "## Use the default eigensystem calculator in numpy as a point of comparison\n",
    "def eigmax_numpy(A):\n",
    "    \"\"\"\n",
    "    Compute the maximum eigenvalue and associated eigenvector in a matrix with Numpy.\n",
    "    \"\"\"\n",
    "    eigsys = np.linalg.eig(A)\n",
    "    ind = np.abs(eigsys[0]).argmax()\n",
    "    return np.real(eigsys[0][ind]), np.real(eigsys[1][:, ind])\n",
    "\n",
    "\n",
    "np.random.seed(2) # for reproducibility\n",
    "mm = np.random.random(size=(10, 10)) / 100\n",
    "mm = np.random.normal(size=(10, 10))# / 100 # these matrices fail to converge more often\n",
    "\n",
    "# mm += mm.T # force hermitian\n",
    "\n",
    "print(np.linalg.cond(mm.T))\n",
    "model = SpectralDecompositionPowerMethod(store_intermediate_results=True, gamma=0.0)\n",
    "model.fit(mm);\n",
    "\n",
    "\n",
    "print(f\"Power method solution: {model.singular_values_}\")\n",
    "print(f\"Numpy solution: {eigmax_numpy(mm)[0]}\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(model.stored_eigenvalues)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Eigenvalue estimate\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(eigmax_numpy(mm)[1], model.components_, '.')\n",
    "plt.xlabel(\"Numpy eigenvector\")\n",
    "plt.ylabel(\"Power method eigenvector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8c3957-096f-49a3-a127-1a040723c598",
   "metadata": {},
   "source": [
    "\n",
    "### Follow-up ideas and additional information\n",
    "\n",
    "+ The runtime complexity of root-finding for a polynomial of the order, $N$ is poorly-defined as a function of $N$, because there are no guaranteed methods for high-order polynomials other than iterative root-finding. A [good guess](https://cs.stackexchange.com/questions/46920/bss-model-computational-complexity-of-finding-the-roots-of-a-polyomial) would be runtime $\\mathcal{O}(N)$ with a large prefactor (for each of $N$ roots, perform a line search, discover a solution, and then reduce the polynomial in degree). However, because finding the initial polynomial via taking a determinant has runtime $\\mathcal{O}(N^3)$, the overall runtime of this method is still unfavorable compared to iterative methods.\n",
    "\n",
    "+ The power method gives us the leading eigenvalue and eigenvector of a matrix. What about the full eigenspectrum of the matrix? A more sophisticated version of the power method is the so-called [QR algorithm](https://en.wikipedia.org/wiki/QR_algorithm). Recall how, at each step, we renormalized our eigenvector estimate. If we instead propagate a bundle of random vectors (in order to estimate all of the different eigenvectors), the equivalent of renormalization would be repeated orthonormalization via a mechanism such as Gram-Schmidt. The QR algorithm basically performs this iterative re-orthonormalization very quickly via the QR factorization of the bundle.\n",
    "\n",
    "+ We just described an iterative algorithm for finding eigenvalues, and we discussed one improvement via the addition of momentum term. It turns out there's lots of other ways to improve our update rule, such as by adjusting the eigenvector estimate at a rate proportional to the previous change (a gradient). Because gradient descent is used to train modern neural networks, there is a lot of research describing the convergence properties of various update rules; [here's a nice list of some common methods](https://ruder.io/optimizing-gradient-descent/index.html). If you want to further improve your power method implementation, try using some of these more modern rules instead.\n",
    "\n",
    "+ We made an analogy between the iterative power method and the action of a linear dynamical system. For nonlinear dynamical systems, [an algorithm very similar to the power method](https://link.springer.com/article/10.1007/BF02128237) is used to calculate the [Lyapunov exponents](http://www.scholarpedia.org/article/Lyapunov_exponent), which quantify the degree of chaos present in the system. On some level, we can think of nonlinear dynamical systems as linear dynamical systems where the elements of the matrix $A$ change with time and position.\n",
    "\n",
    "+ Often in scientific computing we encounter highly-rectangular matrices, $A \\in \\mathbb{R}^{N \\times M}$, $M \\neq N$. For these systems the eigenvectors are not defined, and the schoolyard polynomial equation for the eigenvalues will be overdetermined ($N > M$) or underdetermined ($N < M$). However, we can instead calculate the eigenspectrum of the \"squared\" matrices $A^\\dagger A$ or $A A^\\dagger$, where the dagger indicates the conjugate transpose. We refer to the eigenvectors as the \"right\" or \"left\" singular vectors, respectively, and their associated eigenvalues are called the singular values. All non-square matrices admit a *singular value decomposition* of the form $$ A = U \\Sigma V^\\dagger $$ where the columns of $U$ are the right singular vectors, the columns of $V$ are the left singular vectors, and $\\Sigma$ is a square diagonal matrix with the eigenvalues of $A^\\dagger A$ along the diagonal. Since the eigenvalues and eigenvectors can be listed in any order, by convention we normally list the elements in descending order of eigenvalue magnitude.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa027f6a-5ecd-4a3f-9a4b-7d5ae7cc0345",
   "metadata": {},
   "source": [
    "# 2. Principal Component Analysis and Unsupervised Learning\n",
    "\n",
    "When working with high-dimensional data, it can be helpful to reduce the effective dimensionality of the data by reducing the number of features. For example, suppose we are synthesizing a crystalline material via epitaxy. After preparing a sample, we can measure several properties: reflectivity, conductance, brittleness, etc. After creating a bunch of samples, and measuring several properties for each sample, we want to analyze the relationship among the different features/properties in a systematic way.\n",
    "\n",
    "If we denote a given experiment $i$ with the measurement vector $\\mathbf{x}_i \\in \\mathbb{R}^{N_{feats}}$, then we can represent all of our experiments in a *design matrix* $X \\in \\mathbb{R}^{N_{data} \\times N_{feats}}$, where $N_{data}$ denotes the number of samples or experiments, and $N_{feats}$ represents the number of measurements or features we record for each sample.\n",
    "\n",
    "We know that several of our features are highly correlated across our experiments. Can we find a lower-dimensional representation of our dataset $X' \\in \\mathbb{R}^{N_{data} \\times k}$, $k < N_{feats}$, that describes the majority of variation in our dataset?\n",
    "\n",
    "In principle, reducing the dimensionality of a dataset requires finding an injective function that maps each set of measured features to some lower dimensional set of features,\n",
    "$$\n",
    "\\mathbf{x}' = \\mathbf{f}(\\mathbf{x})\n",
    "$$\n",
    "\n",
    "If this function is linear in the features, $\\mathbf{f}(\\mathbf{x}) = C^T \\mathbf{x}$, then this problem reduces to finding the coefficient matrix $C \\in \\mathbb R^{N_{feats} \\times k}$. For this linear case, we can use Principal Component Analysis (PCA).\n",
    "\n",
    "The basic idea of PCA is that the eigenvectors of a dataset's covariance matrix reveal dominant patterns within the dataset, and so by projecting the dataset onto a subset of these eigenvectors, we can find lower-dimensional representation of the data. PCA is optimal in the sense that the first $k$ principal components represent a unique $k$ dimensional representation of a dataset that captures the most variance in the original data. Because PCA is a linear transform (we project the data on a set of basis vectors), then if we project the original dataset onto the full set of $N_{feats}$ eigenvectors, we basically will have rotated our dataset in a high-dimensional vector space, without discarding any information. [More info about PCA here](http://pmaweb.caltech.edu/~physlab/lab_21_current/Ph21_5_Covariance_PCA.pdf)\n",
    "\n",
    "Mathematically, the steps of computing PCA are relatively simple. We center our data by the feature-wise mean vector, compute the covariance matrix, compute the eigenvectors, sort them in descending order of accompanying eigenvalue magnitude, and then stack the first $k$ eigenvectors to create the coefficient matrix\n",
    "$$\n",
    "X = X_r - \\bar{X_r}\n",
    "$$\n",
    "$$\n",
    "\\Sigma = \\dfrac{1}{N_{data}} X^T X\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{v}_i = \\text{Sort}[\\text{eig}(\\Sigma)]_i        \n",
    "$$\n",
    "$$\n",
    "C = \\{\\mathbf{v}_1, \\mathbf{v}_2, ..., \\mathbf{v}_k \\}\n",
    "$$\n",
    "where $\\text{eig}$ denotes the $i^{th}$ eigenvector of $\\Sigma$. The coefficient matrix $C$ denotes the first $k$ eigenvectors stacked on top of each other, sorted in *decreasing* order of the magnitude of their associated eigenvalue. In this context, we call the eigenvalues the \"singular values\" of the original data matrix $X$, while the eigenvectors are the principal components. The central idea of PCA is that the first $k$ principal components (sorted in descending order of eigenvalue magnitude) represent the optimal variance-preserving $k$ dimensional approximation of the original dataset.\n",
    "\n",
    "Our choice of $k \\leq N_{feats}$ will depend on the properties of the dataset. In practice, we usually plot all of the eigenvalues in descending order of magnitude, and then look for a steep dropoff in their average magnitude---this indicates low dimensionality in the underlying dataset. Deciding the value of $k$ determines whether we favor a more concise and compressed representation, or a more accurate one; and various [heuristics exist](https://arxiv.org/abs/1305.5870) for determining the right threshold. After choosing $k$, we discard the remaining eigenvectors.\n",
    "\n",
    "What's nice about PCA is that it generalizes well even to cases where $N_{feats} \\gg N_{data}$. For example, if we are working with high-resolution images, each pixel is essentially a separate feature, and so $N_{feats} = 2048 \\times 2048 \\sim 10^6$.\n",
    "\n",
    "PCA falls broadly under the category of unsupervised learning, which are machine learning techniques that seek to discover structure and patterns in complex data, without external information like annotations that guide the learning process. Instead, unsupervised learning techniques find alternative representations of datasets that satisfy certain desiderata. In the case of PCA, we find a representation of the data in which the first few discovered variables capture the majority of the dataset's variance, thus compressing meaningful properties of the dataset into a lower-dimensional representation.\n",
    "\n",
    "\n",
    "### The von Karman vortex street\n",
    "\n",
    "As an example dataset, we are going to use the velocity field created as a fluid flow passes over a cylinder. At low flow speeds or high fluid viscosities, the flow parts around the cylinder and then smoothly rejoins behind it. However, as the speed or viscosity decreases, and instability appears in which the flow begins to oscillate behind the cylinder, giving rise to sets of counter-rotating vortices that break off the cylinder and pass into the wake. As speed further increases, this sequence of vortices becomes more chaotic, leading to irregular downstream structure.\n",
    "\n",
    "The control parameter governing this transition from ordered to chaotic flow is the Reynolds number,\n",
    "$$\n",
    "\\text{Re} = \\dfrac{\\rho v L}{\\mu}\n",
    "$$\n",
    "where $\\rho$ is the fluid density, $v$ is the fluid speed, $L$ is the characteristic length of the cylinder, and $\\mu$ is the fluid viscosity. Because $\\rho$ and $\\mu$ are are both material properties of the fluid, they are often combined into a single parameter, the kinematic viscosity $\\nu = \\mu/\\rho$. For water, the kinematic viscosity under standard conditions is $\\nu \\approx 10^{-6} \\text{m}^2/\\text{s}$, and for air, $\\nu \\approx 10^{-5} \\text{m}^2/\\text{s}$. The Reynolds number is a dimensionless quantity that measures the relative importance of inertial forces to viscous forces in the system. In general, flows become more turbulent at higher Reynolds numbers, and they become more frictional at lower Reynolds numbers. For example, the flow through a jet engine can reach $Re \\approx 10^8$, while pouring maple syrup has $Re \\approx 10^{1}$.\n",
    "\n",
    "In the class repository, we have included very large datasets corresponding to time series of snapshots showing the velocity field of the flow past a cylinder. We include separate datasets from several different Reynolds numbers, which show how the structure of the von Karman instability changes as the flow becomes more turbulent.\n",
    "<!-- *For this example problem, you will need to download the [Von Karman Vortex Street Dataset](https://utexas.box.com/s/44f89zfy7v2k4g5wq4kv3vfvkm9w91wm) from Box. Place the downloaded files into the subdirectory `cphy/resources/von_karman_street/`. Alternatively, you can manually edit the path we use below to import the dataset.* -->\n",
    "\n",
    "### To Do\n",
    "\n",
    "1. Download the von Karman dataset and explore it using the code below. What symmetries are present in the data? Do any symmetries change as we increase the Reynolds number? Note: If you are working from a fork of the course repository, the dataset should load automatically. Otherwise, the cells below should automatically download the data sets and place them in higher-level directory `../resources`. If the automatic download fails, you may need to manually download the [von Karman dataset](https://github.com/williamgilpin/cphy/blob/main/resources/von_karman_street/) and place it in the correct directory.\n",
    "\n",
    "Answer: In lower Reynolds numbers, symmetries in our flows are shown through the PC modes being periodic in time.  These vanish at higher Reynolds number as we are going towards a chaotic system.\n",
    "\n",
    "2. Implement Principal Component Analysis in Python. I have included my outline code below; we are going to use multiple inheritence in order to make our implementation compatible with standard conventions for machine learning in Python. I recommend using numpy's built-in eigensystem solvers `np.linalg.eig` and `np.linalg.eigh`\n",
    "\n",
    "Answer: We can use `np.linalg.eigh` as $\\Sigma$ is hermitian.  First it is clearly real, so we just need to see how it is symmetric: $$\\Sigma = \\frac{1}{N_\\text{data}} X^TX \\implies \\Sigma^T = \\frac{1}{N_\\text{data}}\\big(X^TX\\big)^T =  \\frac{1}{N_\\text{data}}X^T(X^T)^T = \\frac{1}{N_\\text{data}}X^TX = \\Sigma.$$\n",
    "\n",
    "3. Plot the eigenvalues of the data covariance matrix in descending order. What does this tell us about the effective dimensionality, and thus optimal number of features, to use to represent the von Karman dataset?\n",
    "\n",
    "Answer: This sharp drop off tells us that we can $\\textit{greatly}$ reduce our number of parameters to describe our system accurately (enough).\n",
    "\n",
    "4. Try re-running your analysis using datasets from different Reynolds numbers. How does the effective dimensionality of the problem change as Reynolds number increases? How is that reflected in the lower-dimensional time series representation?\n",
    "\n",
    "Answer: The effective dimensionality of the problem increases as the Reynolds number increases.  This is seen as the resultant eigenvalues do not drop off as harsh.  That is reflected through more chaotic flows.  More chaotic behavior means less 'trends' to use as our lower dimensionality basis.\n",
    "\n",
    "5. For this problem, the principal components often appear in pairs. Can you think of a reason for this?\n",
    "\n",
    "Answer: If we take any of our principal component plots, another where we swap all of the red/blue components should be just as likely (especially in this system which periodically has these flows about the cylinder).  This is where the pairs come in.\n",
    "\n",
    "6. What happens if we don't subtract the feature-wise mean before calculating PCA? Why does this happen?\n",
    "\n",
    "Answer: The eigenvalues still drop off quickly, just not as fast.  Then when plotting the principal components, they are all shifted by one (in our sorting).  So removing the feature-wise mean will map $PC(n)\\mapsto PC(n-1)$.  Also when plotting our time-series against each other, it no longer is centered about the origin (it oscillates about some other point).  What is probably happening is akin to our first PC mode is the center of mass motion and our second+ PC modes are the relative motions about this COM.  So if we center our data about the origin, this is essentially shifting into the center-of-mass frame to just worry about the finer details of the experiment.\n",
    "\n",
    "7. In Fourier analysis, we project a function onto linear combination of trigonometric basis functions. How is this related to principal components analysis?\n",
    "\n",
    "Answer: That is exacly what we are doing here, in spirit.  We are taking a much more complicated real space problem, and projecting it onto a smaller set of basis functions (complex exponentials, typically) in Fourier space.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e58c1d6",
   "metadata": {},
   "source": [
    "### Load and explore the raw velocity field data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ee6dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # Import the os module\n",
    "import numpy as np # Import the numpy module\n",
    "import urllib.request # Import requests module (downloads remote files)\n",
    "\n",
    "Re = 300 # Reynolds number, change this to 300, 600, 900, 1200\n",
    "\n",
    "fpath = f\"../resources/von_karman_street/vortex_street_velocities_Re_{Re}.npz\"\n",
    "if not os.path.exists(fpath):\n",
    "    print(\"Data file not found, downloading now.\")\n",
    "    print(\"If this fails, please download manually through your browser\")\n",
    "\n",
    "    ## Make a directory for the data file and then download to it\n",
    "    os.makedirs(\"../resources/von_karman_street/\", exist_ok=True)\n",
    "    url = f'https://github.com/williamgilpin/cphy/raw/main/resources/von_karman_street/vortex_street_velocities_Re_{Re}.npz'\n",
    "    urllib.request.urlretrieve(url, fpath)\n",
    "else: \n",
    "    print(\"Found existing data file, skipping download.\")\n",
    "  \n",
    "vfield = np.load(fpath, allow_pickle=True) # Remove allow_pickle=True, as it's not a pickle file\n",
    "print(\"Velocity field data has shape: {}\".format(vfield.shape))\n",
    "\n",
    "# Calculate the vorticity, which is the curl of the velocity field\n",
    "vort_field = np.diff(vfield, axis=1)[..., :-1, 1] + np.diff(vfield, axis=2)[:, :-1, :, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e79c0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "\n",
    "plt.rcParams['animation.embed_limit'] = 2**26\n",
    "\n",
    "# Assuming frames is a numpy array with shape (num_frames, height, width)\n",
    "frames = vort_field.copy()[::4]\n",
    "vmax = np.percentile(np.abs(frames), 98)\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "img = plt.imshow(frames[0], vmin=-vmax, vmax=vmax, cmap=\"RdBu\")\n",
    "plt.xticks([]); plt.yticks([])\n",
    "# tight margins\n",
    "plt.margins(0,0)\n",
    "plt.gca().xaxis.set_major_locator(plt.NullLocator())\n",
    "\n",
    "def update(frame):\n",
    "        img.set_array(frame)\n",
    "\n",
    "ani = FuncAnimation(fig, update, frames=frames, interval=100)\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4829db26",
   "metadata": {},
   "source": [
    "### Implement principal component analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8fc29e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# We are going to use class inheritance to define our object. The two base classes from\n",
    "# scikit-learn represent placeholder objects for working with datasets. They include \n",
    "# many generic methods, like fetching parameters, getting the data shape, etc.\n",
    "# \n",
    "# By inheriting from these classes, we ensure that our object will have access to these\n",
    "# functions, even though we didn't define them ourselves. Earlier in the course\n",
    "# we saw examples where we defined our own template classes. Here, we are using the\n",
    "# template classes define by the scikit-learn Python library.\n",
    "class PrincipalComponents(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    A class for performing principal component analysis on a dataset.\n",
    "\n",
    "    Parameters\n",
    "        random_state (int): random seed for reproducibility\n",
    "        components_ (numpy array): the principal components\n",
    "        singular_values_ (numpy array): the singular values\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, random_state=None):\n",
    "        self.random_state = random_state\n",
    "        self.components_ = None\n",
    "        self.singular_values_ = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Fit the PCA model to the data X. Store the eigenvectors in the attribute\n",
    "        self.components_ and the eigenvalues in the attribute self.singular_values_\n",
    "\n",
    "        NOTE: This method needs to return self in order to work properly with the\n",
    "         scikit-learn base classes from which it inherits.\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): A 2D array of shape (n_samples, n_features) containing the\n",
    "                data to be fit.\n",
    "        \n",
    "        Returns:\n",
    "            self (PrincipalComponents): The fitted object.\n",
    "        \"\"\"\n",
    "\n",
    "        X = X.astype(np.float32) #fix as my version of numpy.linalg doesn't work with 16-bit floats\n",
    "        X = X - np.mean(X,axis=0) #centers data by feature-wise mean vector\n",
    "        covariance = X.T.dot(X)/X.shape[0] \n",
    "        vals,vecs = np.linalg.eigh(covariance) #this automatically sorts eigenvalues/eigenvectors in ascending order\n",
    "\n",
    "        self.singular_values_ = np.flip(vals) #puts eigenvalues in descending order  (reverses array)\n",
    "        self.components_ = np.fliplr(vecs).T  #puts eigenvectors in descending order (left-right flip of matrix, then transpose)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform the data X into the new basis using the PCA components.\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): A 2D array of shape (n_samples, n_features) containing the\n",
    "                data to be transformed.\n",
    "\n",
    "        Returns:\n",
    "            X_new (np.ndarray): A 2D array of shape (n_samples, n_components) containing\n",
    "                the transformed data. n_components <= n_features, depending on whether\n",
    "                we truncated the eigensystem.\n",
    "        \"\"\"\n",
    "        \n",
    "        X = X - np.mean(X,axis=0)\n",
    "        return X.dot(self.components_.T)\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform from principal components space back to the original space\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): A 2D array of shape (n_samples, n_components) containing the\n",
    "                data to be transformed. n_components <= n_features, depending on whether\n",
    "                we truncated the eigensystem.\n",
    "\n",
    "        Returns:\n",
    "            X_new (np.ndarray): A 2D array of shape (n_samples, n_features) containing\n",
    "                the transformed data.\n",
    "        \"\"\"\n",
    "        \n",
    "        return X.dot(self.components_) + np.mean(X,axis=0)\n",
    "\n",
    "    ## You shouldn't need to implement this, because it gets inherited from the base\n",
    "    ## class. But if you are having trouble with the inheritance, you can uncomment\n",
    "    ## this and to implement it.\n",
    "    # def fit_transform(self, X):\n",
    "    #     self.fit(X)\n",
    "    #     return self.transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de6e7a2",
   "metadata": {},
   "source": [
    "### Test and use your code\n",
    "\n",
    "+ If you are working from a local fork of the entire course, then you already have access to the solutions. In this case, make sure to `git pull` to make sure that you are up-to-date (save your work first).\n",
    "+ If you are working from a single downloaded notebook, or are working in Google Colab, then you will need to manually download the solutions file from the course repository. The lines below will do this for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e0a081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "# Check if the \"solutions\" directory exists. If not, create it and download the solution file\n",
    "if not os.path.exists('solutions'):\n",
    "    os.makedirs('solutions')\n",
    "else:\n",
    "    print('Directory \"solutions\" already exists. Skipping creation.')\n",
    "\n",
    "# Now download the solution file into the directory we just created\n",
    "url = 'https://raw.githubusercontent.com/williamgilpin/cphy/main/hw/solutions/pca.py'\n",
    "response = requests.get(url)\n",
    "file_path = os.path.join('solutions', 'sandpile.py')\n",
    "with open(file_path, 'wb') as file:\n",
    "    file.write(response.content)\n",
    "print(f'File saved to {file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d946f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load William's solutions\n",
    "#from solutions.pca import PrincipalComponents\n",
    "\n",
    "data = np.copy(vort_field)[::3, ::2, ::2] # subsample data to reduce compute load\n",
    "data_reshaped = np.reshape(data, (data.shape[0], -1))\n",
    "\n",
    "model = PrincipalComponents()\n",
    "# model = PCA()\n",
    "\n",
    "data_transformed = model.fit_transform(data_reshaped)\n",
    "principal_components = np.reshape(\n",
    "    model.components_, (model.components_.shape[0], data.shape[1], data.shape[2])\n",
    ")\n",
    "\n",
    "## Look at skree plot, and identify the \"elbow\" indicating low dimensionality\n",
    "plt.figure()\n",
    "plt.plot(model.singular_values_[:50])\n",
    "plt.plot(model.singular_values_[:50], '.')\n",
    "plt.xlabel(\"Eigenvalue magnitude\")\n",
    "plt.ylabel(\"Eigenvalue rank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a8d06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Plot the principal components\n",
    "plt.figure(figsize=(20, 10))\n",
    "for i in range(8):\n",
    "    plt.subplot(1, 8, i+1)\n",
    "    vscale = np.percentile(np.abs(principal_components[i]), 99)\n",
    "    plt.imshow(principal_components[i], cmap=\"RdBu\", vmin=-vscale, vmax=vscale)\n",
    "    plt.title(\"PC {}\".format(i+1))\n",
    "\n",
    "## Plot the movie projected onto the principal components\n",
    "plt.figure(figsize=(20, 10))\n",
    "for i in range(8):\n",
    "    plt.subplot(8, 1, i+1)\n",
    "    plt.plot(data_transformed[:, i])\n",
    "    plt.ylabel(\"PC {} Amp\".format(i+1))\n",
    "plt.xlabel(\"Time\")\n",
    "\n",
    "## Plot the time series against each other\n",
    "plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot(data_transformed[:, 0], data_transformed[:, 1], data_transformed[:, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47959017",
   "metadata": {},
   "source": [
    "### Follow-up ideas and additional information\n",
    "\n",
    "+ We are applying PCA to sequences of velocity fields from a fluid flow, but this method can be applied to images, graphs, and almost any high-dimensional dataset. All that really matters is that you can take your data, and apply some sort of invertible transformation so that it becomes a big matrix of shape $(N_{samples}, N_{features})$\n",
    "\n",
    "+ The flow field we are studying was simulated using [Lattice Boltzmann methods](http://www.scholarpedia.org/article/Lattice_Boltzmann_Method). In another module, we will learn how to numerically solve partial differential equations such as the Navier-Stokes equations that govern fluid flows. Lattice Boltzmann methods are distinct from the solvers we will use, however, because they involve simulating individual particles rather than a governing equation. LBM have significant similarity to the cellular automata we explored in previous modules.\n",
    "\n",
    "+ What about the more general problem of finding new coordinates are that *nonlinear* functions of our observed features? One option would be to transform the data with fixed nonlinear functions that capture important features, such as trigonometric functions or spatially-localized kernels, and then apply the PCA calculation in this \"lifted\" space. This approach is the basis of kernel-PCA. Even more general approaches are the subject of current research in nonlinear embedding techniques, such as [UMAP and tSNE](https://pair-code.github.io/understanding-umap/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b4ee79",
   "metadata": {},
   "source": [
    "### Extras\n",
    "\n",
    "This is the code William uses to make videos of his plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fe92b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Re = 1200 # Reynolds number, change this to 300, 600, 900, 1200\n",
    "\n",
    "# Load the two-dimensional velocity field data. Data is stored in a 4D numpy array,\n",
    "# where the first dimension is the time index, the second and third dimensions are the\n",
    "# x and y coordinates, and the fourth dimension is the velocity components (ux or uv).\n",
    "vfield = np.load(\n",
    "    f\"../resources/von_karman_street/vortex_street_velocities_Re_{Re}_largefile.npz\", \n",
    "    allow_pickle=True\n",
    ")\n",
    "print(\"Velocity field data has shape: {}\".format(vfield.shape))\n",
    "\n",
    "# Calculate the vorticity, which is the curl of the velocity field\n",
    "vort_field = np.diff(vfield, axis=1)[..., :-1, 1] + np.diff(vfield, axis=2)[:, :-1, :, 0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493a1f61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f48802e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f69bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0fe123",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c9f12ae",
   "metadata": {},
   "source": [
    "## Future\n",
    "\n",
    "*These are some ideas for future versions of this assignment. They are not part of the homework*\n",
    "\n",
    "\n",
    "### Random matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3c199b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def skew_rot_scale(a):\n",
    "    \"\"\"\n",
    "    Decompose a matrix into translation, skew, rotation, and scale components\n",
    "    \"\"\"\n",
    "    # Translation\n",
    "    t = a[:, -1]\n",
    "    # Rotation and scale\n",
    "    r = a[:-1, :-1]\n",
    "    # Skew\n",
    "    s = 0.5 * (r - r.T)\n",
    "    # Scale\n",
    "    c = 0.5 * (r + r.T)\n",
    "    return t, s, c\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "5d8b3901",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.random(size=(10, 10))\n",
    "a = np.random.normal(size=(10, 10))\n",
    "\n",
    "# a = np.random.random(size=(500, 500))\n",
    "# a = np.random.normal(size=(500, 500))\n",
    "# a = a + a.T\n",
    "# a = a - np.mean(a, axis=1)\n",
    "t, s, c = skew_rot_scale(a)\n",
    "\n",
    "# print(np.linalg.eig(a)[0])\n",
    "# print(np.linalg.norm(c))\n",
    "# print(np.linalg.det(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3d26ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Girko-Ginibri circular law\n",
    "\n",
    "a = np.random.random(size=(500, 500))\n",
    "a = a - np.mean(a, axis=1)\n",
    "\n",
    "# a = np.random.normal(size=(500, 500))\n",
    "\n",
    "# a = a + a.T\n",
    "\n",
    "eig = np.linalg.eig(a)[0]\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(eig.real, eig.imag, '.k', markersize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd0e936",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509824da",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Wigner's Semicircle Law\n",
    "\n",
    "a = np.random.normal(size=(1000, 1000))\n",
    "a = a + a.T\n",
    "\n",
    "eig = np.linalg.eig(a)[0]\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(eig, bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6e080e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make an interactive video (optional; requires ipywidgets and has some RAM overhead)\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual, Layout\n",
    "import ipywidgets as widgets\n",
    "\n",
    "frames = vort_field[::10]\n",
    "\n",
    "vscale = np.percentile(np.abs(vort_field), 98)\n",
    "def plotter(i):\n",
    "    plt.close()\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(\n",
    "        frames[i],\n",
    "        vmin=-vscale, vmax=vscale, cmap=\"RdBu\"\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "interact(\n",
    "    plotter, \n",
    "    i=widgets.IntSlider(0, 0, len(frames) - 1, 1, layout=Layout(width='500px'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb0b2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "vort_field2 = vort_field[::5][:500]\n",
    "vscale = np.percentile(np.abs(vort_field2), 95)\n",
    "\n",
    "for i in range(len(vort_field2) - 1):\n",
    "    \n",
    "    \n",
    "    out_path = \"private_dump/wake/frame\" + str(i).zfill(4) + \".png\"\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(vort_field2[i], cmap=\"RdBu\", vmin=-vscale, vmax=vscale)\n",
    "\n",
    "    plt.xlim([0, vort_field2.shape[-1]])\n",
    "    plt.ylim([60, 60 + vort_field2.shape[-1]])\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.set_axis_off()\n",
    "    ax.xaxis.set_major_locator(plt.NullLocator())\n",
    "    ax.yaxis.set_major_locator(plt.NullLocator())\n",
    "    ax.set_aspect(1, adjustable='box')\n",
    "\n",
    "    plt.savefig(out_path, bbox_inches='tight', pad_inches=0.0, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29318ee0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717962b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('cphy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "0e972983abb2b5c6293c34082f6ff1f6e60e8afbd2a068e0026ccecbb212fdb6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
